\section{Memory Allocator}
	\label{sec:alloc}
Memory allocator is a major interface that operating systems exposed. On Linux, users request memory throught the system call \emph{brk} or
the memory mapped file through \emph{mmap}. Operating systems will ensure
that if the requests are granted, then visiting these address would not cause
protection errors. However, this does not mean operating system will allocate
the physical page, say, setting up the virtual to physical mapping
immediately.  Rather than to fit user's request once and for all, operating
systems may employ a lazy strategy. In this configuration, only when
a page fault occurs, will the operating system check the need to allocate
physical memory. In this section, I try to verify three things.

First is that whether operating system uses on-demand allocation, and second
is when will operating system zero out the pages for security concerns. At last
I will check the benefit of on-demand allocation.

\subsection{Methodology}
To determine whether operating systems allocate pages on demand, I just
allocate a large memory area, and measure the time we walk on it for first
time. It should be quite different from normal walking. To further distinguish
what operating systems are doing, I will measure the time with and without
allocating system call, here I used \emph{mmap}, as well as the normal walk
time.

To check when does operating system zero out the pages, if the allocation is
not done at the system call time, then operating systems then can only: 1.)
choose a known zero page 2.) zero out page on demand. In order to exclude or
include the first approach, I measured the time of allocation with/without
memory pressure. To prevent operating systems to have many free pages, I will
allocate a big memory pool and grab most of the memory. Then I come to check if
the timing in allocation can have some change.  One problem may still remain is
that kernel can zero out pages when pages are unmapped. So I also measure the
normalized time used to unmap a clean page and a dirty page.

\subsection{Experiments}
\subsubsection{Allocation Cost}
\begin{figure}[htpb]
\centering
\includegraphics[width=0.9\linewidth]{../figures/alloc}
\caption{The different part of allocation time. \emph{map} means the entire time
including the mapping, walking, and unmapping. \emph{unmap} has two options, to
zero out the page (suffixed with "-clean"), and don't do extra things (suffixed
with "-no-clean"). The first walk means the duration of the first time we visit
the allocated space, and walk means the time to do memory walk after we have
touched all the pages several times.}
\label{fig:alloc}
\end{figure}
To address questions we mentioned, I measured several parts of the overhead in 
figure~\ref{fig:alloc}:
\begin{itemize}
\item total overhead (tagged as "map-full"). Including mapping, memory walking
and unmapping time.
\item first time walk overhead. This will ensure the pages be allocated.
\item unmapping overhead, with/without reset routine. Measuring this part is
for the sake to explore if operating systems can do zero out at reclaim time.
\item regular memory walk after all the pages are assured to exist in memory(I
monitored the swap space used, and it's not grown at any experiment time).
\end{itemize}
To compare the zero-out strategy, we also measured the allocation time when
memory is sparse  resource. In this scenario, the free memory I leave to
operating system is less than 1.5GB, and the allocation unit I used is 1GB, to
make sure few free pages are in the system, and it must zero out pages at the
time of granting or revoking them to users.

From figure~\ref{fig:alloc} we can find out the allocation is on demand, since
regular walk is much cheaper comparing to first time walk right after mmap
returns. And also the performance retains regardless of the memory pressure.
According to that fact, it is only possible to zero out pages on allocation
and deallocation time. But the measurement of unmapping time did not show much
information, since the unmapping time is sufficiently large that we can not
tell if it actually does page zero out or not.

\subsubsection{Benefit from On-Demand Allocation}

The on demand allocation is an optmization for virtual memory. To see its
effect, I compared the cost of random walking and sequential walking on the
128MB newly allocated memory. For space limitation, I only present the result
on 4B stride (or 32M random read) as in figure~\ref{fig:on-demand}. Further
results are uploaded to~\cite{github}, it is consistent with my conclusions
here.

\begin{figure}[htb]
\centering
\includegraphics[width=0.9\linewidth]{../figures/on_demand_4k}
\caption{The time of doing random walk and sequential walk through the 
newly allocated 128MB memory. For small page space, random walk would not bring
in the page not needed, thus performs better than sequential walk. But for
large page space, random walk is slower.}
\label{fig:on-demand}
\end{figure}

Figure~\ref{fig:on-demand} exhibits interesting result: random walk runs
faster with small pages, but slower with large page. In the former case,
not bring in unnecessary pages saves time, while in the later one there
are only a few pages to bring in, and poor locality slows it down.

\subsection{Discussion}
One thing hard to illustrate is whether operating system will 'smartly'
allocate some pages for small requests. For example, when user requests for
only 1 page, then allocate the page immediately to avoid a page fault can be
somewhat a good choice. This period is too small to measure correctly just by
using wall clock. However, if the operating system can provide a page fault
counter for each process, then it is still possible to measure such behavior.
